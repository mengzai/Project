<!DOCTYPE html>
<html>
<head>
<title>Logistic Regression</title>
<style type="text/css">
body{width:1280px; margin:auto; font-family:'Lucida Console','宋体'; font-size:10px;}
h3{text-align:center;margin:8px 0; color:red;}
h4{margin:8px 0; color:red;}
h5{margin:8px 0; color:black;}
p{text-align:justify; text-justify:inter-ideograph; word-wrap:break-word; line-height:120%; margin:8px 0;}
span.keyr{font-weight:bold; color:red;}
span.keyg{font-weight:bold; color:green;}
span.keyb{font-weight:bold; color:blue;}
span.keyk{font-weight:bold; color:black;}
pre{color:gray; text-align:justify; text-justify:inter-ideograph; white-space:pre-wrap; word-wrap:break-word; }
</style>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
<h3>Logistic Regression</h3>

<h4>1. 原理介绍</h4>

<p>
分类模型可以分为概率模型和非概率模型，分别用条件概率分布 \(P(Y|X)\) 或决策函数 \(Y=f(X)\) 表示，其中 \(X\) 和 \(Y\) 分别为输入和输出的随机变量。
逻辑回归，属于概率模型 \(P(Y|X)\)，可以用来解决两类分类问题和多类分类问题。
</p>

<p>
以两类分类问题为例，假设类别标号\(Y\in\{+1,-1\}\)，则样本属于正类的概率定义为
$$ P(y=+1|\mathbf{x})=\frac{1}{1+\exp(-(\mathbf{w}^T\mathbf{x}+b))} $$
其中\(\mathbf{w}\)是权重向量，\(\mathbf{x}\)是特征向量，\(b\)是偏差项；样本属于负类的概率定义为
$$ P(y=-1|\mathbf{x})=1-P(y=+1|\mathbf{x})=\frac{1}{1+\exp(+(\mathbf{w}^T\mathbf{x}+b))} $$
两者可以统一表达为
$$ P(y|\mathbf{x})=\frac{1}{1+\exp(-y(\mathbf{w}^T\mathbf{x}+b))}. $$
</p>

<p>
给定\(n\)个样本 \(\{(\mathbf{x}_1,y_1), (\mathbf{x}_2,y_2), ..., (\mathbf{x}_n,y_n)\}\)，参数的最大似然估计定义为
$$ (\mathbf{w},b) = \mathop{\arg\max}_{\mathbf{w},b}\prod_{i=1}^n P(y_i|\mathbf{x}_i) $$
取负对数\(-\log(.)\)后，问题转化为
$$ (\mathbf{w},b) = \mathop{\arg\min}_{\mathbf{w},b}\sum_{i=1}^n -\log(P(y_i|\mathbf{x}_i)) $$
$$ (\mathbf{w},b) = \mathop{\arg\min}_{\mathbf{w},b}\sum_{i=1}^n \log(1+\exp(-y_i(\mathbf{w}^T\mathbf{x}_i+b))) $$
</p>

<p>
令 \( f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}+b \)，则LR的损失函数形式为 \(L(y,f(\mathbf{x})) = \log(1+\exp(-y \cdot f(\mathbf{x})))\)。
令 \( t=y\cdot f(\mathbf{x}) \)，损失函数 \(\log(1+\exp(-t))\) 随 \(t\) 的变化曲线为<br>
<center><img src='logistic_regression_figure1.png' width='40%' /></center>
它是 \(\max(0,-t)\) 的近似函数，函数是凸函数，优化问题是凸优化问题，局部极值解是全局最优解。
</p>

<p>为避免过学习，增强模型的泛化能力，常在优化目标函数中添加正则项。</p>

<p>当添加\(L_2\)范数时，新的目标优化损失函数定义为
$$ (\mathbf{w},b) = \mathop{\arg\min}_{\mathbf{w},b} \frac{\lambda}{2}\sum_{i=1}^d w_i^2 + \frac{1}{n}\sum_{i=1}^n \log(1+\exp(-y_i(\mathbf{w}^T\mathbf{x}_i+b))) $$
参数\(\lambda\)为正则项权重因子，正则项缩小特征的权重差异，得到平滑解。
</p>

<p>
当添加\(L_1\)范数时，新的目标优化损失函数定义为
$$ (\mathbf{w},b) = \mathop{\arg\min}_{\mathbf{w},b} \lambda\sum_{i=1}^d|w_i| + \frac{1}{n}\sum_{i=1}^n \log(1+\exp(-y_i(\mathbf{w}^T\mathbf{x}_i+b))) $$
参数\(\lambda\)为正则项权重因子，正则项消除相关性较小特征，得到稀疏解。
</p>

<p>
多类分类问题，假设有\(c\)个不同类别，LR优化如下问题
$$ {\{(\mathbf{w}_k,b_k)\}}_{k=1}^c = \mathop{\arg\min}_{\mathbf{w},b} \frac{\lambda}{2}\sum_{i=1}^c\sum_{j=1}^d w_{ij}^2 + \frac{1}{n}\sum_{i=1}^n \log\left(\frac{\exp(\mathbf{w}_i^T\mathbf{x}+b)}{}+1+\exp(-y_i(\mathbf{w}^T\mathbf{x}_i+b))\right) $$
</p>

<h4>2. 最优化求解</h4>

<p>当采用 \(L_2\) 范数时，损失函数的优化求解适合L-BFGS算法。L-BFGS算法常用于非线性无约束的最优化问题，一般要求优化目标函数连续可导，关于L-BFGS的原理和实现请参考最优化部分的文档。</p>

<p>当采用 \(L_1\) 范数时，损失函数的优化求解适合OWL-QN算法。OWL-QN算法是单象限的L-BFGS算法，每次迭代都在同一象限内，可以到达象限边界，下次迭代可进入另外一个象限。关于OWL-QN的原理和实现请参考最优化部分的文档。</p>

<h4>3. 使用方法</h4>
<p>LR由一个C++类实现，lr.h/lr.cpp，外部依赖于Data类和LBFGS类。train_lr.cpp是训练模型的例子，test_lr.cpp是测试模型的例子。提供了g++编译的makefile文件，使用方式如下</p>
<h5>(1) 在终端命令行下，cd到machine_learning/classification/logitic_regression下，依次输入命令make clean, make，生成 train_lr 和 test_lr 两个可执行程序。</h5>
<pre>
$ make clean
rm -f *.o train_lr test_lr
$ make
g++ -Wno-logical-op-parentheses -O2 -o train_lr train_lr.cpp lr.cpp -I ../data -I ../../optimization ../data/data.cpp ../../optimization/lbfgs.cpp
g++ -Wno-logical-op-parentheses -O2 -o test_lr test_lr.cpp lr.cpp -I ../data -I ../../optimization ../data/data.cpp ../../optimization/lbfgs.cpp
$ ls
lr.cpp  lr.h  makefile model.lr test_lr  test_lr.cpp train.txt train_lr train_lr.cpp
</pre>
<h5>(2) 训练LR模型的例子，执行 ./train_lr ...，需输入4个参数，用法如下</h5>
<pre>
$ ./train_lr train.txt 2 0.001 model.lr
load samples: num = 4, dim = 2
iter =   1, f = 0.57600192, t = 1.0000, gd = -0.12500000
iter =   2, f = 0.13157855, t = 1.0000, gd = -0.76285306
iter =   3, f = 0.07697215, t = 1.0000, gd = -0.07276166
iter =   4, f = 0.04548885, t = 1.0000, gd = -0.04592034
iter =   5, f = 0.03507039, t = 1.0000, gd = -0.01499591
iter =   6, f = 0.03176871, t = 1.0000, gd = -0.00503329
iter =   7, f = 0.03118149, t = 1.0000, gd = -0.00096011
iter =   8, f = 0.03113806, t = 1.0000, gd = -0.00007814
iter =   9, f = 0.03113739, t = 1.0000, gd = -0.00000130
iter =  10, f = 0.03113739, t = 1.0000, gd = -0.00000000
 
L-BFGS iterations = 11, function evalutions = 11
training time = 0.000 s
 
wb =      4.6651
          4.6651
          0.0000
--------------------------------------------
            真实正例    真实负例      准确率
预测正例           2           0     100.00%
预测负例           0           2     100.00%
召回率       100.00%     100.00%     100.00%
--------------------------------------------
</pre>
<h5>(3) 测试LR模型的例子，执行 ./test_lr ...，需输入2个参数，用法如下</h5>
<pre>
$ ./test_lr
Usage: test_lr test_file model
       e.g. test_lr test.txt model.lr
$ ./test_lr test.txt model.lr
load samples: num = 4, dim = 2
--------------------------------------------
            真实正例    真实负例      准确率
预测正例           2           0     100.00%
预测负例           0           2     100.00%
召回率       100.00%     100.00%     100.00%
--------------------------------------------
</pre>

<h3>Logistic Regression with Non-negative Weights</h3>
<h4>1. 求解问题</h4>
<p>在有些情况，特征有明确的物理含义，要求分类器的权重都是非负的系数，构成可解释的模型。
<pre>
例子一，每一维特征都表示某种距离，要求一种非负加权的距离表示，使得正负样本的分类效果最好。
例子二，每一维特征都是某个分类器的正类类别条件概率的输出，要求一种非负加权的概率表示，使得正负样本的分类效果最好。
</pre>
LRNW通过最优化如下问题，
$$ \begin{eqnarray} && (\mathbf{w},b) = \mathop{\arg\min}_{\mathbf{w},b} \frac{\lambda}{2}\sum_{i=1}^d w_i^2 + \frac{1}{n}\sum_{i=1}^n \log(1+\exp(-y_i(\mathbf{w}^T\mathbf{x}_i+b))) \\
&& \text{s.t.} \quad w_i \ge 0,\ \forall i=1,...,d.
\end{eqnarray} $$
可以求解得到一个线性分类器\( f(\mathbf{x})= w_1x_1+w_2x_2+\cdots+w_dx_d+b\)，其中\(w_i\ge 0\)且\(b\in R\)。
</p>

<h4>2. L-BFGS-B</h4>
<p>The L-BFGS-B algorithm extends L-BFGS to handle simple box constraints (aka bound constraints) on variables; that is, constraints of the form li ≤ xi ≤ ui where li and ui are per-variable constant lower and upper bounds, respectively (for each xi, either or both bounds may be omitted). The method works by identifying fixed and free variables at every step (using a simple gradient method), and then using the L-BFGS method on the free variables only to get higher accuracy, and then repeating the process.</p> 
<p>References</p>
<pre>
R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound Constrained Optimization, (1995), SIAM Journal on Scientific and Statistical Computing , 16, 5, pp. 1190-1208.
C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization (1997), ACM Transactions on Mathematical Software, Vol 23, Num. 4, pp. 550 - 560.
J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization (2011), to appear in ACM Transactions on Mathematical Software.

https://github.com/stephenbeckr/L-BFGS-B-C
http://users.iems.northwestern.edu/~nocedal/lbfgsb.html
</pre>

<h4>3. 扩展的LR和使用方法</h4>
<p>扩展的LR支持非负权重的LR模型的训练，有三个文件train_lr，test_lr和lbfgsb.so。用法如下
<pre>
Usage: train_lr train_file option(0:LR, 1:L1-LR, 2:L2-LR, 3:LR+, 4:L2-LR+) lambda model
       e.g. train_lr train.txt 2 0.001 model.lr
</pre>
其中LR+代表非负权重的LR，L2-LR+代表含二范数正则项的非负权重的LR。
</p>
<h5>3.1 使用option=2，L2-LR时的训练例子</h5>
<pre>
$ train_lr train_nnw.txt 2 0.001 model.lr
load samples: num = 4, dim = 2
iter = 1, f = 0.63995278, t = 1.0000, gd = -0.05640625
iter = 2, f = 0.20992872, t = 1.0000, gd = -0.67795754
iter = 3, f = 0.10325422, t = 1.0000, gd = -0.16991931
iter = 4, f = 0.06814159, t = 1.0000, gd = -0.05154009
iter = 5, f = 0.05709585, t = 1.0000, gd = -0.01634337
iter = 6, f = 0.05441052, t = 1.0000, gd = -0.00423815
iter = 7, f = 0.05408248, t = 1.0000, gd = -0.00055825
iter = 8, f = 0.05406788, t = 1.0000, gd = -0.00002709
iter = 9, f = 0.05406776, t = 1.0000, gd = -0.00000023
L-BFGS iterations = 10, function evalutions = 10
training time = 0.000 s
wb = 8.4610
    -0.3425
    -3.8394
--------------------------------------------
             真实正例      真实负例       准确率
预测正例            2            0     100.00%
预测负例            0            2     100.00%
召回率        100.00%      100.00%     100.00%
--------------------------------------------
</pre>

<h5>3.2 使用option=4，L2-LR+时的训练例子</h5>
<pre>
$ train_lr train_nnw.txt 4 0.001 model.lr
load samples: num = 4, dim = 2
training time = 0.000 s
wb = 8.4792
     0.0000
    -3.9856
--------------------------------------------
             真实正例      真实负例       准确率
预测正例            2            0     100.00%
预测负例            0            2     100.00%
召回率        100.00%      100.00%     100.00%
--------------------------------------------
</pre>

</body>
</html>