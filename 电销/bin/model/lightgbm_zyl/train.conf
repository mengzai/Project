# task type, support train and predict
task = train

# boosting type, support gbdt for now, alias: boosting, boost
boosting_type = gbdt

# application type, support following application
# regression , regression task
# binary , binary classification task
# lambdarank , lambdarank task
# alias: application, app
objective = binary

# eval metrics, support multi metric, delimite by ',' , support following metrics
# l1 
# l2 , default metric for regression
# ndcg , default metric for lambdarank
# auc 
# binary_logloss , default metric for binary
# binary_error
metric = auc

# frequence for metric output
metric_freq = 1

# true if need output metric for training data, alias: tranining_metric, train_metric
is_training_metric = true

# number of bins for feature bucket, 255 is a recommend setting, it can save memories, and also has good accuracy. 
max_bin = 255
#bin_construct_sample_cnt = 5000000

# training data
# if exsting weight file, should name to "binary.train.weight"
# alias: train_data, train
data = jieqing_train

weight = name:weight
#scale_pos_weight = 0.5

# validation data, support multi validation data, separated by ','
# if exsting weight file, should name to "binary.test.weight"
# alias: valid, test, test_data, 
valid_data = jieqing_test_1,jieqing_test_10

header = true

#ignore_column = name:datasource


# number of trees(iterations), alias: num_tree, num_iteration, num_iterations, num_round, num_rounds
num_trees = 100

# shrinkage rate , alias: shrinkage_rate
learning_rate = 0.05
#drop_rate = 0.03
early_stopping_round = 50


# number of leaves for one tree, alias: num_leaf
num_leaves =3
#num_leaves = 127

# round for early stopping
#early_stopping = 30

# type of tree learner, support following types:
# serial , single machine version
# feature , use feature parallel to train
# data , use data parallel to train
# voting , use voting based parallel to train
# alias: tree
tree_learner = serial
#tree_learner = voting

# number of threads for multi-threading. One thread will use one CPU, defalut is setted to #cpu. 
# num_threads = 8

# feature sub-sample, will random select 80% feature to train on each iteration 
# alias: sub_feature
#feature_fraction = 0.5

# Support bagging (data sub-sample), will perform bagging every 5 iterations
#bagging_freq = 1

# Bagging farction, will random select 80% data on bagging
# alias: sub_row
#bagging_fraction = 0.5

# minimal number data for one leaf, use this to deal with over-fit
# alias : min_data_per_leaf, min_data
min_data_in_leaf = 100

# minimal sum hessians for one leaf, use this to deal with over-fit
min_sum_hessian_in_leaf = 50 #5.0

# save memory and faster speed for sparse feature, alias: is_sparse
#is_enable_sparse = true

# when data is bigger than memory size, set this to true. otherwise set false will have faster speed
# alias: two_round_loading, two_round
use_two_round_loading = false

# true if need to save data to binary file and application will auto load data from binary file next time
# alias: is_save_binary, save_binary
is_save_binary_file = false

# output model file
output_model = task1_model.txt

# support continuous train from trained gbdt model
# input_model= trained_model.txt

# output prediction file for predict task
# output_result= prediction.txt

# support continuous train from initial score file
# input_init_score= init_score.txt


# number of machines in parallel training, alias: num_machine
num_machines = 1

# local listening port in parallel training, alias: local_port
local_listen_port = 12400

# machines list file for parallel training, alias: mlist
machine_list_file = mlist.txt

#header = true

#categorical_feature = 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143
#categorical_feature = name:os_release,app_cat1_10


# bin_construct_sample_cnt, default=50000, type=int
# Number of data that sampled to construct histogram bins.
# Will give better training result when set this larger. But will increase data loading time.
# Set this to larger value if data is very sparse.
#bin_construct_sample_cnt = 300000

# is_unbalance, default=false, type=bool
# used in binary classification. Set this to true if training data are unbalance.
#is_unbalance = true
